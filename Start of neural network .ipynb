{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9a3eb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import _pickle as Pickle\n",
    "import gzip\n",
    "import os.path\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29cba024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numba in c:\\users\\henrik\\anaconda3\\lib\\site-packages (0.54.1)\n",
      "Requirement already satisfied: numpy<1.21,>=1.17 in c:\\users\\henrik\\anaconda3\\lib\\site-packages (from numba) (1.20.3)\n",
      "Requirement already satisfied: llvmlite<0.38,>=0.37.0rc1 in c:\\users\\henrik\\anaconda3\\lib\\site-packages (from numba) (0.37.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\henrik\\anaconda3\\lib\\site-packages (from numba) (58.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numba-scipy in c:\\users\\henrik\\anaconda3\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: numba>=0.45 in c:\\users\\henrik\\anaconda3\\lib\\site-packages (from numba-scipy) (0.54.1)\n",
      "Requirement already satisfied: scipy<=1.6.2,>=0.16 in c:\\users\\henrik\\anaconda3\\lib\\site-packages (from numba-scipy) (1.6.2)\n",
      "Requirement already satisfied: llvmlite<0.38,>=0.37.0rc1 in c:\\users\\henrik\\anaconda3\\lib\\site-packages (from numba>=0.45->numba-scipy) (0.37.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\henrik\\anaconda3\\lib\\site-packages (from numba>=0.45->numba-scipy) (58.0.4)\n",
      "Requirement already satisfied: numpy<1.21,>=1.17 in c:\\users\\henrik\\anaconda3\\lib\\site-packages (from numba>=0.45->numba-scipy) (1.20.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numba\n",
    "%pip install numba-scipy\n",
    "from numba import jit, uint64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6824abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#First try at creating a neural nett, used code from https://github.com/MichalDanielDobrzanski/DeepLearningPython but have\n",
    "# reached a dead end at the moment, someting wrong with the dot product i'm doing. Gonna pick up another time. \n",
    "\n",
    "\n",
    "f = gzip.open(\"mnist_expanded.pkl.gz\", 'rb')\n",
    "    \n",
    "imported = Pickle.Unpickler(file=f, encoding='latin1')\n",
    "training_data, validation_data, test_data = imported.load()\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d20add51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Network(object):\n",
    " \n",
    "    def __init__(self, sizes, weights = None, biases = None):\n",
    "        self.sizes = sizes \n",
    "        self.biases =  [np.random.randn(y, 1) for y in sizes[1:]] #creates an array of weights randomized from the standard normal function\n",
    "        self.weights = [np.random.randn(y, x) for y,x in zip(sizes[:-1], sizes[1:])] #same but takes in the number of neurons in one layer and creates weights connection to each neuron in the next\n",
    "        \n",
    "        if weights:\n",
    "            self.weights = weights \n",
    "        else:\n",
    "            print(\"hi\")\n",
    "        if biases: \n",
    "            self.biases = biases \n",
    "        else:\n",
    "            print(\"hi\")\n",
    "            \n",
    "    def feedforward(self, a): #takes in a which is a list of inputs from any layer in the neural nett. \n",
    "        for b, w in zip(self.biases, self.weights): #zips an tuple consisting of the bias and weigths for a neuron and then \n",
    "            n_a = np.zeros(len(w[1]))\n",
    "            \n",
    "            for k in range(len(b)):\n",
    "                n_a[k] += b[k]\n",
    "            print(len(w))\n",
    "            for i in range(len(w)):\n",
    "                j=0\n",
    "                for items in w[i]:\n",
    "                    \n",
    "                    \n",
    "                    n_a[j]= n_a[j] + items*a[i]\n",
    "                    j+=1\n",
    "            \n",
    "                \n",
    "            a = n_a\n",
    "            a = sigmoid(a)\n",
    "          \n",
    "          \n",
    "            \n",
    "            #and then applies the sigmoid function, could perhaps use a different function here. \n",
    "            #print(np.shape(a))\n",
    "        return a   \n",
    "    \n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size , eta, test_data = None):\n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data) #creates opreable data\n",
    "        \n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "            n_test = len(test_data) \n",
    "        \n",
    "        for j in range(epochs ):\n",
    "            random.shuffle(training_data) \n",
    "            mini_batches = [\n",
    "            training_data[k:k+mini_batch_size] # divides the training data into smaller portions\n",
    "            for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                print(mini_batch)\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch {} : {} / {}\", format(j,self.evaluate(test_data),n_test))\n",
    "            else:\n",
    "                print(\"Epoch {} complete\", (j))\n",
    "                \n",
    "   \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "    \n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "        \n",
    "       \n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "  \n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "    \n",
    "  \n",
    "    def evaluate_one(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        \n",
    "        \n",
    "        test_results = self.feedforward(test_data)\n",
    "        return test_results\n",
    "    \n",
    "    \n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "    \n",
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9db8fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "net = Network([784, 30, 10])\n",
    "#net.SGD(training_data, 10, 30, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95e89a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_b = list(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44ef18a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the soulution is 4\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "a =int(random.random()*10000)\n",
    "\n",
    "test_image = training_data[0]\n",
    "test_image_answer = training_data[1] \n",
    "test_image_answer = test_image_answer[a]\n",
    "test_image = test_image[a]\n",
    "\n",
    "\n",
    "data = np.zeros((28,28))\n",
    "i = 0\n",
    "for x in range(28):\n",
    "    for y in range(28):\n",
    "        data[x,y]=test_image[i]\n",
    "        i+=1\n",
    "        \n",
    "        \n",
    "plt.figure()\n",
    "\n",
    "plt.title(\"pixel_plot\")\n",
    "plt.imshow(data, cmap='Greys', interpolation='nearest') \n",
    "plt.colorbar()\n",
    "plt.savefig('pixel_plot.png')\n",
    "plt.show()\n",
    "print(\"the soulution is\", test_image_answer)\n",
    "\n",
    "\n",
    "print(len(result_b))\n",
    "\n",
    "#result  = net.evaluate(result_b) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0151bdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = net.weights \n",
    "biases = net.biases \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a02e7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "print(len(weights[1]))\n",
    "print(len(biases[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb0b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [2,2,2,2]\n",
    "a = [np.random.randn(y, x) for y,x in zip(sizes[:-1], sizes[1:])]\n",
    "b = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "#print(a)\n",
    "c =list(zip(b,a))\n",
    "print(c[0])test_data1 = test_data[random]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
